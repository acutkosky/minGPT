{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some shakespear, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i:i+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/20/2020 19:49:24 - INFO - mingpt.model -   number of parameters: 6.280960e+05\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=3, n_head=4, n_embd=128)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 16: train loss 2.89596. lr 9.999394e-04: 100%|██████████| 17/17 [01:05<00:00,  3.85s/it]\n",
      "epoch 2 iter 16: train loss 2.62343. lr 9.997555e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 3 iter 16: train loss 2.53097. lr 9.994483e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 4 iter 16: train loss 2.48344. lr 9.990178e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 5 iter 16: train loss 2.45037. lr 9.984642e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 6 iter 16: train loss 2.42800. lr 9.977876e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 7 iter 16: train loss 2.41430. lr 9.969882e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 8 iter 16: train loss 2.40052. lr 9.960661e-04: 100%|██████████| 17/17 [00:59<00:00,  3.47s/it]\n",
      "epoch 9 iter 16: train loss 2.38215. lr 9.950216e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 10 iter 16: train loss 2.36815. lr 9.938550e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 11 iter 16: train loss 2.35964. lr 9.925665e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 12 iter 16: train loss 2.34919. lr 9.911564e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 13 iter 16: train loss 2.33958. lr 9.896252e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 14 iter 16: train loss 2.33274. lr 9.879731e-04: 100%|██████████| 17/17 [00:59<00:00,  3.53s/it]\n",
      "epoch 15 iter 16: train loss 2.32180. lr 9.862006e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 16 iter 16: train loss 2.31076. lr 9.843082e-04: 100%|██████████| 17/17 [00:59<00:00,  3.49s/it]\n",
      "epoch 17 iter 16: train loss 2.30119. lr 9.822962e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 18 iter 16: train loss 2.29830. lr 9.801653e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 19 iter 16: train loss 2.28848. lr 9.779158e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 20 iter 16: train loss 2.27979. lr 9.755485e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 21 iter 16: train loss 2.26951. lr 9.730637e-04: 100%|██████████| 17/17 [00:59<00:00,  3.49s/it]\n",
      "epoch 22 iter 16: train loss 2.25889. lr 9.704623e-04: 100%|██████████| 17/17 [00:59<00:00,  3.47s/it]\n",
      "epoch 23 iter 16: train loss 2.23999. lr 9.677447e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 24 iter 16: train loss 2.23048. lr 9.649118e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 25 iter 16: train loss 2.22989. lr 9.619641e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 26 iter 16: train loss 2.21364. lr 9.589024e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 27 iter 16: train loss 2.20184. lr 9.557275e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 28 iter 16: train loss 2.18913. lr 9.524401e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 29 iter 16: train loss 2.18124. lr 9.490411e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 30 iter 16: train loss 2.16585. lr 9.455313e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 31 iter 16: train loss 2.15453. lr 9.419115e-04: 100%|██████████| 17/17 [00:59<00:00,  3.53s/it]\n",
      "epoch 32 iter 16: train loss 2.14740. lr 9.381827e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 33 iter 16: train loss 2.14114. lr 9.343458e-04: 100%|██████████| 17/17 [01:00<00:00,  3.54s/it]\n",
      "epoch 34 iter 16: train loss 2.12933. lr 9.304017e-04: 100%|██████████| 17/17 [00:59<00:00,  3.49s/it]\n",
      "epoch 35 iter 16: train loss 2.12211. lr 9.263514e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 36 iter 16: train loss 2.10957. lr 9.221959e-04: 100%|██████████| 17/17 [00:59<00:00,  3.49s/it]\n",
      "epoch 37 iter 16: train loss 2.10496. lr 9.179362e-04: 100%|██████████| 17/17 [00:59<00:00,  3.48s/it]\n",
      "epoch 38 iter 16: train loss 2.10355. lr 9.135734e-04: 100%|██████████| 17/17 [00:59<00:00,  3.48s/it]\n",
      "epoch 39 iter 16: train loss 2.09779. lr 9.091085e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 40 iter 16: train loss 2.08232. lr 9.045427e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 41 iter 16: train loss 2.08317. lr 8.998770e-04: 100%|██████████| 17/17 [01:00<00:00,  3.55s/it]\n",
      "epoch 42 iter 16: train loss 2.07372. lr 8.951127e-04: 100%|██████████| 17/17 [00:59<00:00,  3.53s/it]\n",
      "epoch 43 iter 16: train loss 2.06412. lr 8.902509e-04: 100%|██████████| 17/17 [00:59<00:00,  3.53s/it]\n",
      "epoch 44 iter 16: train loss 2.04993. lr 8.852927e-04: 100%|██████████| 17/17 [00:59<00:00,  3.49s/it]\n",
      "epoch 45 iter 16: train loss 2.04600. lr 8.802396e-04: 100%|██████████| 17/17 [00:59<00:00,  3.48s/it]\n",
      "epoch 46 iter 16: train loss 2.04446. lr 8.750925e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 47 iter 16: train loss 2.03352. lr 8.698530e-04: 100%|██████████| 17/17 [01:00<00:00,  3.53s/it]\n",
      "epoch 48 iter 16: train loss 2.03176. lr 8.645221e-04: 100%|██████████| 17/17 [01:00<00:00,  3.54s/it]\n",
      "epoch 49 iter 16: train loss 2.02037. lr 8.591013e-04: 100%|██████████| 17/17 [00:59<00:00,  3.53s/it]\n",
      "epoch 50 iter 16: train loss 2.02788. lr 8.535919e-04: 100%|██████████| 17/17 [00:59<00:00,  3.50s/it]\n",
      "epoch 51 iter 16: train loss 2.00970. lr 8.479953e-04: 100%|██████████| 17/17 [01:00<00:00,  3.54s/it]\n",
      "epoch 52 iter 16: train loss 2.01937. lr 8.423127e-04: 100%|██████████| 17/17 [01:00<00:00,  3.53s/it]\n",
      "epoch 53 iter 16: train loss 2.00991. lr 8.365458e-04: 100%|██████████| 17/17 [01:00<00:00,  3.53s/it]\n",
      "epoch 54 iter 16: train loss 1.99937. lr 8.306957e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 55 iter 16: train loss 1.99133. lr 8.247641e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 56 iter 16: train loss 1.99114. lr 8.187523e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 57 iter 16: train loss 1.98928. lr 8.126619e-04: 100%|██████████| 17/17 [01:01<00:00,  3.59s/it]\n",
      "epoch 58 iter 16: train loss 1.98358. lr 8.064943e-04: 100%|██████████| 17/17 [01:00<00:00,  3.55s/it]\n",
      "epoch 59 iter 16: train loss 1.98002. lr 8.002511e-04: 100%|██████████| 17/17 [01:00<00:00,  3.53s/it]\n",
      "epoch 60 iter 16: train loss 1.98220. lr 7.939338e-04: 100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "epoch 61 iter 16: train loss 1.97224. lr 7.875439e-04: 100%|██████████| 17/17 [01:00<00:00,  3.58s/it]\n",
      "epoch 62 iter 16: train loss 1.96972. lr 7.810832e-04: 100%|██████████| 17/17 [00:59<00:00,  3.52s/it]\n",
      "epoch 63 iter 16: train loss 1.96191. lr 7.745530e-04: 100%|██████████| 17/17 [01:00<00:00,  3.55s/it]\n",
      "epoch 64 iter 16: train loss 1.96363. lr 7.679551e-04: 100%|██████████| 17/17 [01:00<00:00,  3.54s/it]\n",
      "epoch 65 iter 16: train loss 1.96127. lr 7.612911e-04: 100%|██████████| 17/17 [01:02<00:00,  3.65s/it]\n",
      "epoch 66 iter 16: train loss 1.95476. lr 7.545626e-04: 100%|██████████| 17/17 [01:06<00:00,  3.93s/it]\n",
      "epoch 67 iter 16: train loss 1.95303. lr 7.477713e-04: 100%|██████████| 17/17 [01:06<00:00,  3.92s/it]\n",
      "epoch 68 iter 16: train loss 1.94664. lr 7.409189e-04: 100%|██████████| 17/17 [01:17<00:00,  4.57s/it]\n",
      "epoch 69 iter 16: train loss 1.94111. lr 7.340070e-04: 100%|██████████| 17/17 [01:24<00:00,  4.96s/it]\n",
      "epoch 70 iter 16: train loss 1.93734. lr 7.270373e-04: 100%|██████████| 17/17 [01:24<00:00,  4.99s/it]\n",
      "epoch 71 iter 16: train loss 1.93274. lr 7.200117e-04: 100%|██████████| 17/17 [01:24<00:00,  4.94s/it]\n",
      "epoch 72 iter 16: train loss 1.93552. lr 7.129317e-04: 100%|██████████| 17/17 [01:23<00:00,  4.91s/it]\n",
      "epoch 73 iter 16: train loss 1.92603. lr 7.057992e-04: 100%|██████████| 17/17 [01:25<00:00,  5.03s/it]\n",
      "epoch 74 iter 16: train loss 1.92791. lr 6.986160e-04: 100%|██████████| 17/17 [01:18<00:00,  4.61s/it]\n",
      "epoch 75 iter 11: train loss 1.78297. lr 6.934821e-04:  71%|███████   | 12/17 [00:48<00:19,  3.85s/it]"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=200, batch_size=512, learning_rate=0.001,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=4)#, weight_decay=0.0)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alright, let's sample some character-level shakespear\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=0.5, sample=True, top_k=5)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
